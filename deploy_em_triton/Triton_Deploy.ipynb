{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5794f5ee",
   "metadata": {},
   "source": [
    "## Deploy de modelo com o Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fa374",
   "metadata": {},
   "source": [
    "## Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743a9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libs:\n",
    "!pip3 install tritonclient\\[all\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1b11382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo_rede_neural\n",
      "|____.DS_Store\n",
      "|____1\n",
      "| |______pycache__\n",
      "| | |____model.cpython-310.pyc\n",
      "| |____model.py\n",
      "| |____model_vec.pkl\n",
      "| |____model_net\n",
      "| | |____fingerprint.pb\n",
      "| | |____variables\n",
      "| | | |____variables.data-00000-of-00001\n",
      "| | | |____variables.index\n",
      "| | |____saved_model.pb\n",
      "| | |____assets\n",
      "|____config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "#!tree modelo_rede_neural\n",
    "!find modelo_rede_neural -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0f0b7",
   "metadata": {},
   "source": [
    "Como o modelo utilizado foi desenvolvido com Rede Neural, a estrutura foi salva utilizando o formado SavedModel que acaba por criar uma substrutura de pastas.\n",
    "Outro detalhe é que foram salvos dois modelos um para Rede Neural (SavedModel) e outro para o TFIDF (.plk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05867e6f",
   "metadata": {},
   "source": [
    "## Config.pbxt - Protobuffer\n",
    "O arquivo de de configuração do config.pbtxt especifica as entradas e saídas dos modelos.\n",
    "No caso, a entrada é uma string é uma ou mais strings e a saída é uma probabilidade da forma de sring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02f09f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: \"python\"\n",
      "\n",
      "input {\n",
      "    name: \"ENTRADA\"\n",
      "    data_type: TYPE_STRING\n",
      "    dims: [ 1 ]\n",
      "}\n",
      "\n",
      "output {\n",
      "    name: \"PREDICAO\"\n",
      "    data_type: TYPE_STRING\n",
      "    dims: [ 1 ]\n",
      "}\n",
      "\n",
      "instance_group [{ kind: KIND_CPU }]\n"
     ]
    }
   ],
   "source": [
    "!cat modelo_rede_neural/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abf8e4",
   "metadata": {},
   "source": [
    "## Model.py \n",
    "- O arquivo model.py contém o código para carregar o modelo com base nas configurações fornecidas pelo protobuffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd829f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import numpy as np\n",
      "import triton_python_backend_utils as pb_utils\n",
      "from joblib import load\n",
      "import tensorflow as tf\n",
      "\n",
      "\n",
      "class TritonPythonModel:\n",
      "    def initialize(self, args):\n",
      "        self.model_config = model_config = json.loads(args['model_config'])\n",
      "\n",
      "        predicao_config = pb_utils.get_output_config_by_name(\n",
      "            model_config, \"PREDICAO\")\n",
      "        \n",
      "        self.predicao_dtype = pb_utils.triton_string_to_numpy(\n",
      "            predicao_config['data_type'])\n",
      "\n",
      "        version_path =  args['model_repository'] + '/' + args['model_version']\n",
      "\n",
      "        self.vectorize = load(version_path + '/model_vec.pkl')\n",
      "        self.model = tf.saved_model.load(version_path + '/model_net')\n",
      "        self.model.predict = self.model.signatures['serving_default']\n",
      "\n",
      "    def execute(self, requests):\n",
      "        responses = []\n",
      "\n",
      "        for request in requests:\n",
      "            in_x = pb_utils.get_input_tensor_by_name(request, \"ENTRADA\")\n",
      "\n",
      "            input_x = in_x.as_numpy()\n",
      "            input_x = self.vectorize.transform(input_x).toarray()\n",
      "            output_x = self.model(input_x.tolist())\n",
      "            predicao_tensor = pb_utils.Tensor(\"PREDICAO\", output_x.numpy().astype(self.predicao_dtype))\n",
      "\n",
      "            inference_response = pb_utils.InferenceResponse(\n",
      "                output_tensors=[predicao_tensor])\n",
      "            responses.append(inference_response)\n",
      "\n",
      "        return responses\n",
      "\n",
      "    def finalize(self):\n",
      "        print('Cleaning up...')\n"
     ]
    }
   ],
   "source": [
    "!cat modelo_rede_neural/1/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62585c81",
   "metadata": {},
   "source": [
    "## Deploy do modelo na triton usando o podman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55792a2b",
   "metadata": {},
   "source": [
    "Imagem do triton mais recente: 23.09-py3\n",
    "# Imagem do triton mais recente: 24.03-py3\n",
    "`podman pull nvcr.io/nvidia/tritonserver:23.09-py3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbaa038",
   "metadata": {},
   "source": [
    "No terminal, executar o comando:\n",
    "\n",
    "`podman run --rm -p 8000:8000 -v $HOME/Documents/repositorio_git_estudos/triton:/models nvcr.io/nvidia/tritonserver:24.03-py3 /bin/bash -c \"pip install -r /models/requirements.txt && tritonserver --model-repository=/models\"`\n",
    "\n",
    "\n",
    "O que faz esse comado:\n",
    "\n",
    "- Executa um contêiner usando o podman, mapeando a porta 8000 do host para a porta 8000 do contêiner e montando o diretório $HOME/Documents/repositorio_git_estudos/triton do host para /models dentro do contêiner. \n",
    "- Instala o requirements (bibliotecas necessárias) para o modelo, neste caso foram utilizadas a biblioteca tensorflow e scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cd360",
   "metadata": {},
   "source": [
    "## Após o deploy, vamos realizar inferências!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fcffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williansmagalhaesprimo/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_name\":\"modelo_rede_neural\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"PREDICAO\",\"datatype\":\"BYTES\",\"shape\":[1,1],\"data\":[\"0.999789834022522\"]}]}\n",
      "Fake news\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "url = \"http://localhost:8000/v2/models/modelo_rede_neural/versions/1/infer\"\n",
    "\n",
    "\n",
    "# input data\n",
    "input_data = np.array(['it is a fake news']).reshape(1)\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"name\": \"ENTRADA\",\n",
    "      \"shape\": input_data.shape,\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": input_data.tolist()\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "print(response.text)\n",
    "# Map the request in Fake or Real news\n",
    "response = float(json.loads(response.text)[\"outputs\"][0][\"data\"][0])\n",
    "response = 'Fake news' if response > .5  else  'Real news'\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702ac68",
   "metadata": {},
   "source": [
    "Para verificar se o modelo estava respondendo adequadamente, foram realizados alguns acionamentos por meio da URL local enviando um string.\n",
    "Após o retorno (response), é feito um mapeamento de probabilidade: Acima de 50% é considerado Fake news e abaixo é considerado uma Notícia real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0ed15",
   "metadata": {},
   "source": [
    "## Comandinhos de verificação\n",
    "Os comandos a seguir são apenas para validar se o servidor está ativo e os modelos estão disponíveis para inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c40dca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um cliente para se comunicar com o Triton\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "triton_client = httpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b5d88ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers {}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar se o servidor está ativo para receber solicitações\n",
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ad8e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/ready, headers {}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar se o Triton está pronto para receber inferências\n",
    "triton_client.is_server_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9adcb788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/models/modelo_rede_neural, headers {}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '190'}>\n",
      "bytearray(b'{\"name\":\"modelo_rede_neural\",\"versions\":[\"1\"],\"platform\":\"python\",\"inputs\":[{\"name\":\"ENTRADA\",\"datatype\":\"BYTES\",\"shape\":[1]}],\"outputs\":[{\"name\":\"PREDICAO\",\"datatype\":\"BYTES\",\"shape\":[1]}]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'modelo_rede_neural',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'python',\n",
       " 'inputs': [{'name': 'ENTRADA', 'datatype': 'BYTES', 'shape': [1]}],\n",
       " 'outputs': [{'name': 'PREDICAO', 'datatype': 'BYTES', 'shape': [1]}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metadados do modelo \n",
    "triton_client.get_model_metadata(\"modelo_rede_neural\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
